{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Given the following configration of NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.85  0.86  0.28]\n",
      "Hinge loss 1.58\n",
      "[ 1.81109075  0.19985822  0.45174902]\n",
      "Cross entorp loss 0.451749024431\n"
     ]
    }
   ],
   "source": [
    "W = np.array([[0.01,-0.05, 0.1, 0.05],\n",
    "            [0.7,0.2,0.05,0.16],\n",
    "            [0, -0.45, -0.2, 0.03]])\n",
    "\n",
    "x = np.array([-15,22, -44, 56])\n",
    "b = np.array([0, 0.2, -0.3])\n",
    "\n",
    "o = W.dot(x)+b\n",
    "print(o)\n",
    "# Hinge loss (SVM)\n",
    "L_h = max(0,o[0]-o[2]+1)+max(0, o[1]-o[2]+1)\n",
    "print(\"Hinge loss\",L_h)\n",
    "# croos-entropy loss (Softmax)\n",
    "S = np.exp(o)/np.exp(o).sum()\n",
    "L_ce= -np.log10(S)\n",
    "print(L_ce)\n",
    "print(\"Cross entorp loss\",L_ce[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove W23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.85  0.86 -1.4 ]\n",
      "Hinge loss 3.26\n",
      "[ 1.6638603   0.05262778  1.03413331]\n",
      "Cross entorp loss 1.03413330552\n"
     ]
    }
   ],
   "source": [
    "W = np.array([[0.01,-0.05, 0.1, 0.05],\n",
    "            [0.7,0.2,0.05,0.16],\n",
    "            [0, -0.45, -0.2, 0]])\n",
    "\n",
    "\n",
    "x = np.array([-15,22, -44, 56])\n",
    "b = np.array([0, 0.2, -0.3])\n",
    "\n",
    "o = W.dot(x)+b\n",
    "print(o)\n",
    "# Hinge loss (SVM)\n",
    "L_h = max(0,o[0]-o[2]+1)+max(0, o[1]-o[2]+1)\n",
    "print(\"Hinge loss\",L_h)\n",
    "# croos-entropy loss (Softmax)\n",
    "S = np.exp(o)/np.exp(o).sum()\n",
    "L_ce= -np.log10(S)\n",
    "print(L_ce)\n",
    "print(\"Cross entorp loss\",L_ce[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model after we prune one weight W20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.85  0.86  0.28]\n",
      "Hinge loss 1.58\n",
      "[ 1.81109075  0.19985822  0.45174902]\n",
      "Cross entorp loss 0.451749024431\n"
     ]
    }
   ],
   "source": [
    "W = np.array([[0.01,-0.05, 0.1, 0.05],\n",
    "            [0.7,0.2,0.05,0.16],\n",
    "            [0, -0.45, -0.2, 0.03]])\n",
    "\n",
    "\n",
    "x = np.array([-15,22, -44, 56])\n",
    "b = np.array([0, 0.2, -0.3])\n",
    "\n",
    "o = W.dot(x)+b\n",
    "print(o)\n",
    "# Hinge loss (SVM)\n",
    "L_h = max(0,o[0]-o[2]+1)+max(0, o[1]-o[2]+1)\n",
    "print(\"Hinge loss\",L_h)\n",
    "# croos-entropy loss (Softmax)\n",
    "S = np.exp(o)/np.exp(o).sum()\n",
    "L_ce= -np.log10(S)\n",
    "print(L_ce)\n",
    "print(\"Cross entorp loss\",L_ce[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning another weight w11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.85 -3.54 -1.22]\n",
      "Hinge loss 0\n",
      "[ 0.81990246  1.11956565  0.11200246]\n",
      "Cross entorp loss 0.112002455795\n"
     ]
    }
   ],
   "source": [
    "W = np.array([[0.01,-0.05, 0.1, 0.05],\n",
    "            [0.7,0,0.05,0.16],\n",
    "            [0.1, -0.45, -0.2, 0.03]])\n",
    "\n",
    "\n",
    "x = np.array([-15,22, -44, 56])\n",
    "b = np.array([0, 0.2, -0.3])\n",
    "\n",
    "o = W.dot(x)+b\n",
    "print(o)\n",
    "# Hinge loss (SVM)\n",
    "L_h = max(0,o[0]-o[2]+1)+max(0, o[1]-o[2]+1)\n",
    "print(\"Hinge loss\",L_h)\n",
    "# croos-entropy loss (Softmax)\n",
    "S = np.exp(o)/np.exp(o).sum()\n",
    "L_ce= -np.log10(S)\n",
    "print(L_ce)\n",
    "print(\"Cross entorp loss\",L_ce[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning another weight w01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.75  0.86 -1.22]\n",
      "Hinge loss 3.55\n",
      "[ 1.21213386  0.07862527  0.98195779]\n",
      "Cross entorp loss 0.981957789334\n"
     ]
    }
   ],
   "source": [
    "W = np.array([[0.01,0, 0.1, 0.05],\n",
    "            [0.7,0.2,0.05,0.16],\n",
    "            [0.1, -0.45, -0.2, 0.03]])\n",
    "\n",
    "\n",
    "x = np.array([-15,22, -44, 56])\n",
    "b = np.array([0, 0.2, -0.3])\n",
    "\n",
    "o = W.dot(x)+b\n",
    "print(o)\n",
    "# Hinge loss (SVM)\n",
    "L_h = max(0,o[0]-o[2]+1)+max(0, o[1]-o[2]+1)\n",
    "print(\"Hinge loss\",L_h)\n",
    "# croos-entropy loss (Softmax)\n",
    "S = np.exp(o)/np.exp(o).sum()\n",
    "L_ce= -np.log10(S)\n",
    "print(L_ce)\n",
    "print(\"Cross entorp loss\",L_ce[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning another weight w21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -2.85   0.86  10.18]\n",
      "Hinge loss 0\n",
      "[  5.65889697e+00   4.04766444e+00   3.98696331e-05]\n",
      "Cross entorp loss 3.98696331418e-05\n"
     ]
    }
   ],
   "source": [
    "W = np.array([[0.01,-0.05, 0.1, 0.05],\n",
    "            [0.7,0.2,0.05,0.16],\n",
    "            [0, 0, -0.2, 0.03]])\n",
    "\n",
    "x = np.array([-15,22, -44, 56])\n",
    "b = np.array([0, 0.2, -0.3])\n",
    "\n",
    "o = W.dot(x)+b\n",
    "print(o)\n",
    "# Hinge loss (SVM)\n",
    "L_h = max(0,o[0]-o[2]+1)+max(0, o[1]-o[2]+1)\n",
    "print(\"Hinge loss\",L_h)\n",
    "# croos-entropy loss (Softmax)\n",
    "S = np.exp(o)/np.exp(o).sum()\n",
    "L_ce= -np.log10(S)\n",
    "print(L_ce)\n",
    "print(\"Cross entorp loss\",L_ce[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning another weight w00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.7   0.86  0.28]\n",
      "Hinge loss 1.58\n",
      "[ 1.74703106  0.20094271  0.4528335 ]\n",
      "Cross entorp loss 0.452833504627\n"
     ]
    }
   ],
   "source": [
    "W = np.array([[0,-0.05, 0.1, 0.05],\n",
    "            [0.7,0.2,0.05,0.16],\n",
    "            [0, -0.45, -0.2, 0.03]])\n",
    "\n",
    "x = np.array([-15,22, -44, 56])\n",
    "b = np.array([0, 0.2, -0.3])\n",
    "\n",
    "o = W.dot(x)+b\n",
    "print(o)\n",
    "# Hinge loss (SVM)\n",
    "L_h = max(0,o[0]-o[2]+1)+max(0, o[1]-o[2]+1)\n",
    "print(\"Hinge loss\",L_h)\n",
    "# croos-entropy loss (Softmax)\n",
    "S = np.exp(o)/np.exp(o).sum()\n",
    "L_ce= -np.log10(S)\n",
    "print(L_ce)\n",
    "print(\"Cross entorp loss\",L_ce[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.45"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
